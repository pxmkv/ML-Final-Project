





import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import ast
import seaborn as sns
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import statsmodels.formula.api as smf
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
from datetime import datetime
from dateutil import parser
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, GRU, LSTM, Embedding
from keras.optimizers import Adam


cpi = pd.read_csv('https://pkgstore.datahub.io/core/cpi/cpi_csv/data/04cb8fe18892497287d23e20d0e1ceb9/cpi_csv.csv')
cpi = cpi.drop(columns = 'Country Code')
cpi = cpi.loc[cpi['Year'] >= 1986]
cpi = cpi.loc[cpi['Country Name'] == 'United States']
cpi = cpi.drop(columns = 'Country Name')
cpi = cpi.set_index('Year')
cpi.head()





movies = pd.read_csv('movies 2.csv')
movies = movies.loc[movies['year'] <= 2014]
movies = movies.loc[movies['country'] == 'United States']

movies = movies.rename(columns={'year': 'Year'})
movies


# combining the CPI and movie datasets
merged_df = pd.merge(movies, cpi, on='Year')


nan_values = merged_df.isna().sum()
print(nan_values)





merged_df['success'] = (merged_df['gross'] / merged_df['budget']).apply(lambda x: 1 if x > 2 else 0)
merged_df_copy = merged_df.copy() # for imputing budget for CART

merged_df = merged_df.dropna(subset=['budget'])
nan_count = merged_df['score'].isna().sum()

merged_df.head()


nan_values = merged_df.isna().sum()
print(nan_values)





# Converting 'budget' to numeric
merged_df['budget'] = pd.to_numeric(merged_df['budget'], errors='coerce')

# Previewing the cleaned dataset
merged_df.head()


nan_values = merged_df.isna().sum()
print(nan_values)


from dateutil import parser

merged_df['released'] = merged_df['released'].astype(str).apply(lambda x: parser.parse(x, fuzzy=True))

merged_df['year'] = merged_df['released'].dt.year
merged_df['month'] = merged_df['released'].dt.month
merged_df['day'] = merged_df['released'].dt.day

merged_df.head()


nan_values = merged_df.isna().sum()
print(nan_values)


merged_df.dropna(inplace=True)
nan_values = merged_df.isna().sum()
print(nan_values)


# converting day of the week
from datetime import datetime
merged_df['released'] = pd.to_datetime(merged_df['released'])

def get_day_of_week(date):
    return date.strftime('%A')

merged_df['dayofweek'] = merged_df['released'].apply(get_day_of_week)

print(merged_df['dayofweek'])


# create variables which measures who the top stars, companies, etc are

top_stars = merged_df['star'].value_counts().head(100)
merged_df['star_studded'] = 0
merged_df.loc[merged_df['star'].isin(top_stars.index), 'star_studded'] = 1


top_company = merged_df['company'].value_counts().head(20)
merged_df['top_company'] = 0
merged_df.loc[merged_df['company'].isin(top_company.index), 'top_company'] = 1

top_director = merged_df['director'].value_counts().head(20)
merged_df['top_director'] = 0
merged_df.loc[merged_df['director'].isin(top_company.index), 'top_director'] = 1

top_writer = merged_df['writer'].value_counts().head(20)
merged_df['top_writer'] = 0
merged_df.loc[merged_df['writer'].isin(top_company.index), 'top_writer'] = 1

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
merged_df['rating'] = label_encoder.fit_transform(merged_df['rating'])
merged_df['genre'] = label_encoder.fit_transform(merged_df['genre'])
merged_df['company'] = label_encoder.fit_transform(merged_df['company'])
merged_df['dayofweek'] = label_encoder.fit_transform(merged_df['dayofweek'])
merged_df['star'] = label_encoder.fit_transform(merged_df['star'])
merged_df['director'] = label_encoder.fit_transform(merged_df['director'])
merged_df['writer'] = label_encoder.fit_transform(merged_df['writer'])

merged_df


all_columns = merged_df.columns.tolist()
all_columns
# columns we should include in all models when starting should include rating, genre, score, votes, CPI, budget, runtime, CPI, year, month, day, dayofweek, star_studded, top_company, top_director, top_writer





import ast
import matplotlib.pyplot as plt
import seaborn as sns

# Setting the style of seaborn
sns.set(style="whitegrid")

# 1. Distribution of Numerical Features
fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 12))
sns.histplot(data=merged_df, x='budget', bins=30, ax=axes[0, 0], kde=True, color='blue')
axes[0, 0].set_title('Distribution of Budget')
sns.histplot(data=merged_df, x='gross', bins=30, ax=axes[0, 1], kde=True, color='green')
axes[0, 1].set_title('Distribution of Gross Revenue')
sns.histplot(data=merged_df, x='runtime', bins=30, ax=axes[1, 0], kde=True, color='red')
axes[1, 0].set_title('Distribution of Runtime')
plt.tight_layout()
plt.show()






import plotly.express as px
import plotly.graph_objs as go
from plotly.offline import init_notebook_mode, iplot, plot
#init_notebook_mode(connected=True)

fig = px.scatter(merged_df, x='budget', y='gross', hover_data=['name'], color='genre', width=900, height=900)
fig.update_layout(
    title='The Relationship between Budget and Revenue',
    xaxis_title='Budget',
    yaxis_title='Revenue',
    font=dict(
        size=12
    )
)
iplot(fig)





time_series_df=merged_df.copy()
time_series_df=time_series_df[['released','gross']]
time_series_df['released'] = pd.to_datetime(time_series_df['released'])
time_series_df = time_series_df.sort_values(by='released')
time_series_df['year'] = time_series_df['released'].dt.year
avg_gross_per_year = time_series_df.groupby('year')['gross'].mean()
avg_gross_per_year_df = avg_gross_per_year.reset_index()
avg_gross_per_year_df.columns = ['year', 'avg_gross']


year = avg_gross_per_year_df['year']

print(year.unique())


def plot_entire_series(x, y, tittle='',**kwargs):
    
    plt.figure(figsize=(16, 4))
    plt.plot(x, y, linewidth=2, color='black')
    for key, value in kwargs.items():
        plt.plot(x, value, linewidth=2, color=key)
    plt.xlabel('Date', fontsize=16)
    plt.ylabel('Sales', fontsize=16)
    plt.title(tittle)
    plt.show()
    return None
def plot_last_fifty(x, y, **kwargs):
    
    plt.figure(figsize=(16, 4))
    plt.scatter(x[-50:], y[-50:], linewidth=3, color='black')
    plt.plot(x[-50:], y[-50:], linewidth=3, color='black')
    
    for key, value in kwargs.items():
        plt.plot(x[-50:], value[-50:], linewidth=3, color=key)
        
    plt.xlabel('Date', fontsize=16)
    plt.ylabel('Sales', fontsize=16)
    plt.show()
    
    return None
def OSR2(model, X_test, y_test, y_train):
    y_pred = model.predict(X_test)
    SSE = np.sum((y_test - y_pred)**2)
    SST = np.sum((y_test - np.mean(y_train))**2)
    return (1 - SSE/SST)


plot_entire_series(avg_gross_per_year_df['year'],avg_gross_per_year_df['avg_gross'],'Average gross for each year')



time_train = avg_gross_per_year_df[avg_gross_per_year_df['year'] < 2010]
time_test  = avg_gross_per_year_df[avg_gross_per_year_df['year'] >= 2010]
time_train['AvgGrossLastYear'] = time_train['avg_gross'].shift(1)
time_test['AvgGrossLastYear'] = time_test['avg_gross'].shift(1)
x = time_train['year'].to_numpy()
y = time_train['avg_gross'].to_numpy()
print(time_train.shape,time_test.shape)



plot_entire_series(x,y,'Training set average gross for each year')





ar1 = smf.ols(formula='avg_gross ~ AvgGrossLastYear', data=time_train).fit()
print(ar1.summary())


y_pred = ar1.predict(time_train)
plot_entire_series(x, y, red=y_pred)
plot_last_fifty(x, y, red=y_pred)


y_pred = ar1.predict(time_test)
x_test = time_test['year'].to_numpy()
y_test = time_test['avg_gross'].to_numpy()
plot_last_fifty(x_test, y_test, green=y_pred)


print('Auto-regressive Model OSR2:', round(OSR2(ar1, time_test, y_test, y), 5))





time_train['AvgGrossLastTwoYear'] = time_train['avg_gross'].shift(2)
time_test['AvgGrossLastTwoYear'] = time_test['avg_gross'].shift(2)
ar2 = smf.ols(formula='avg_gross ~ AvgGrossLastYear + AvgGrossLastTwoYear', data=time_train).fit()
print(ar2.summary())


y_pred = ar2.predict(time_test)
x_test = time_test['year'].to_numpy()
y_test = time_test['avg_gross'].to_numpy()
plot_last_fifty(x_test, y_test, green=y_pred)


print('Higher-order Auto-regressive Model OSR2:', round(OSR2(ar2, time_test, y_test, y), 5))


from sklearn.cluster import KMeans
from sklearn.preprocessing import MinMaxScaler





from sklearn.model_selection import train_test_split
merged_df['gross_log'] = np.log(merged_df['gross'])
merged_df

# Split the data into training and testing sets
train, test = train_test_split(merged_df, test_size=0.3, random_state=42)





import statsmodels.formula.api as smf

#should we of genre and/or rating ... some of them were statistically sig, others were not.

model = smf.ols(formula='gross_log ~ CPI + budget + score',
                 data=train).fit()
print(model.summary())




# Import the library that contains all the functions/modules related to the regression model
import statsmodels.api as sm

from statsmodels.stats.outliers_influence import variance_inflation_factor

def VIF(df, columns):
    values = sm.add_constant(df[columns]).values
    num_columns = len(columns)+1
    vif = [variance_inflation_factor(values, i) for i in range(num_columns)]
    return pd.Series(vif[1:], index=columns)


cols = ['CPI', 'budget', 'score']
VIF(train, cols)

# when we ran VIF, a lot of the movies didn't have a budget included because of (blank) reasons. Our first thoughts were to do score, budget and CPI





model = smf.ols(formula='success ~ CPI + budget + score',
                 data=train).fit()
print(model.summary())


merged_df











from sklearn.ensemble import RandomForestRegressor

regressor_df = merged_df.copy()
# Regressor because y is gross log and is continuous, need to use regressor

# X = regressor_df[['rating', 'genre', 'score', 'votes', 'budget', 'company', 'runtime','CPI', 'month', 'dayofweek', 'top_company', 'star_studded']]
X = regressor_df[['rating', 'genre', 'score', 'votes', 'budget', 'runtime', 'CPI', 'year', 'month', 'day', 'dayofweek', 'star_studded', 'top_company', 'top_director', 'top_writer']]
y = regressor_df['gross_log']
X_train_gross, X_test_gross, y_train_gross, y_test_gross = train_test_split(X, y, test_size=0.2, random_state=42)
rf_regressor = RandomForestRegressor(n_estimators = 100, random_state = 42)
rf_regressor.fit(X_train_gross, y_train_gross)
regressor_predictions = rf_regressor.predict(X_test_gross)
regressor_predictions[:10]


y_train_gross


rating_counts = merged_df['rating'].value_counts().sort_index()
print(rating_counts)


genre_counts = merged_df['genre'].value_counts().sort_index()
print(genre_counts)


company_counts = merged_df['company'].value_counts().sort_index()
company_counts.head(20)


runtime_counts = merged_df['runtime'].value_counts().sort_index()
runtime_counts.head(20)


maxruntime = merged_df['runtime'].max()
maxruntime


from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

mae = mean_absolute_error(y_test_gross, regressor_predictions)
mse = mean_squared_error(y_test_gross, regressor_predictions)
rmse = mean_squared_error(y_test_gross, regressor_predictions, squared=False)
r_squared = r2_score(y_test_gross, regressor_predictions)

print(f"Mean Absolute Error: {mae}")
print(f"Mean Squared Error: {mse}")
print(f"Root Mean Squared Error: {rmse}")
print(f"R-squared: {r_squared}")


feature_importance = rf_regressor.feature_importances_
feature_importance_with_columns = list(zip(X.columns, rf_regressor.feature_importances_))

sorted_feature_importance = sorted(feature_importance_with_columns, key=lambda x: x[1], reverse=True)

for feature, importance in sorted_feature_importance:
    print(f"Feature: {feature}, Importance: {importance}")


feature_importance = rf_regressor.feature_importances_
sorted_idx = feature_importance.argsort()
features = ['rating', 'genre', 'score', 'votes', 'budget', 'runtime', 'CPI', 'year', 'month', 'day', 'dayofweek', 'star_studded', 'top_company', 'top_director', 'top_writer']
plt.figure(figsize=(10, 8))
plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')
plt.yticks(range(len(sorted_idx)), [features[i] for i in sorted_idx])
plt.xlabel('Feature Importance')
plt.title('Feature Importance in Random Forest Regressor Predicting Gross Log')
plt.show()





from sklearn.model_selection import RandomizedSearchCV

param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [50, 100, 150],
    'min_samples_split': [5, 10, 15],
    'min_samples_leaf': [1, 5, 10]
}

random_search = RandomizedSearchCV(estimator=rf_regressor, param_distributions=param_grid,
                                   scoring='neg_mean_squared_error', cv=5, n_jobs=-1, n_iter=20)

random_search.fit(X_train_gross, y_train_gross)
best_params = random_search.best_params_
best_score = random_search.best_score_

print(f"Best Parameters: {best_params}")
print(f"Best Negative Mean Squared Error: {best_score}")


# Using RF Regressor with the Best Parameters found from Grid Search
rf_best_regressor = RandomForestRegressor(n_estimators=100, min_samples_split=5, min_samples_leaf=5, max_depth=50)

# rf_best_regressor = RandomForestRegressor(n_estimators=150, min_samples_split=10, min_samples_leaf=5, max_depth=150) # got it using different random forest regressor for grid cv

rf_best_regressor.fit(X_train_gross, y_train_gross)

reg_grid_predictions = rf_best_regressor.predict(X_test_gross)

mae = mean_absolute_error(y_test_gross, reg_grid_predictions)
mse = mean_squared_error(y_test_gross, reg_grid_predictions)
rmse = mean_squared_error(y_test_gross, reg_grid_predictions, squared=False)
r_squared = r2_score(y_test_gross, reg_grid_predictions)

print(f"Mean Absolute Error: {mae}")
print(f"Mean Squared Error: {mse}")
print(f"Root Mean Squared Error: {rmse}")
print(f"R-squared: {r_squared}")


feature_importance = rf_best_regressor.feature_importances_
feature_importance_with_columns = list(zip(X.columns, rf_best_regressor.feature_importances_))

sorted_feature_importance = sorted(feature_importance_with_columns, key=lambda x: x[1], reverse=True)

for feature, importance in sorted_feature_importance:
    print(f"Feature: {feature}, Importance: {importance}")


feature_importance = rf_best_regressor.feature_importances_
sorted_idx = feature_importance.argsort()
features = ['rating', 'genre', 'score', 'votes', 'budget', 'runtime', 'CPI', 'year', 'month', 'day', 'dayofweek', 'star_studded', 'top_company', 'top_director', 'top_writer']
plt.figure(figsize=(10, 8))
plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')
plt.yticks(range(len(sorted_idx)), [features[i] for i in sorted_idx])
plt.xlabel('Feature Importance')
plt.title('Feature Importance in Random Forest Regressor Predicting Gross Log with Grid Search CV')
plt.show()





from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

merged_copy = merged_df.copy()
# because y is now binary, use classifier
# X = merged_copy[['rating', 'genre', 'score', 'votes', 'budget', 'company', 'runtime','CPI', 'month', 'dayofweek', 'top_company', 'star_studded']]
X = merged_copy[['rating', 'genre', 'score', 'votes', 'CPI', 'budget', 'runtime', 'year', 'month', 'day', 'dayofweek', 'star_studded', 'top_company', 'top_director', 'top_writer']]
y = merged_copy['success']
X_train_success, X_test_success, y_train_success, y_test_success = train_test_split(X, y, test_size=0.2, random_state=42)
rf_classifier = RandomForestClassifier(n_estimators = 100, random_state = 42)
rf_classifier.fit(X_train_success, y_train_success)
predictions_classifier = rf_classifier.predict(X_test_success)
predictions_classifier[:10]


success_accuracy = accuracy_score(y_test_success, predictions_classifier)
print("Accuracy: ", success_accuracy)


feature_importance = rf_classifier.feature_importances_
feature_importance_with_columns = list(zip(X.columns, rf_classifier.feature_importances_))

sorted_feature_importance = sorted(feature_importance_with_columns, key=lambda x: x[1], reverse=True)

for feature, importance in sorted_feature_importance:
    print(f"Feature: {feature}, Importance: {importance}")


feature_importance = rf_classifier.feature_importances_
sorted_idx = feature_importance.argsort()
features = ['rating', 'genre', 'score', 'votes', 'budget', 'runtime', 'CPI', 'year', 'month', 'day', 'dayofweek', 'star_studded', 'top_company', 'top_director', 'top_writer']
plt.figure(figsize=(10, 8))
plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')
plt.yticks(range(len(sorted_idx)), [features[i] for i in sorted_idx])

plt.xlabel('Feature Importance')
plt.title('Feature Importance in Random Forest Classifier Predicting Success')
plt.show()






from sklearn.model_selection import RandomizedSearchCV

param_grid = {
    'n_estimators': [50, 100, 150],
    'max_depth': [50, 100, 150],
    'min_samples_split': [5, 10, 15],
    'min_samples_leaf': [1, 5, 10]
}

random_search = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_grid,
                                   scoring='accuracy', cv=5, n_jobs=-1, n_iter=20)

random_search.fit(X_train_success, y_train_success)
best_params = random_search.best_params_
best_score = random_search.best_score_

print(f"Best Parameters: {best_params}")
print(f"Accuracy: {best_score}")


rf_best_classifier = RandomForestClassifier(n_estimators=100, min_samples_split=10, min_samples_leaf=1, max_depth=50)
rf_best_classifier.fit(X_train_success, y_train_success)
pred_class = rf_classifier.predict(X_test_success)
best_accuracy_classifier = accuracy_score(y_test_success, pred_class)
print("Accuracy: ", best_accuracy_classifier)


feature_importance = rf_best_classifier.feature_importances_
feature_importance_with_columns = list(zip(X.columns, rf_best_classifier.feature_importances_))
sorted_feature_importance = sorted(feature_importance_with_columns, key=lambda x: x[1], reverse=True)

for feature, importance in sorted_feature_importance:
    print(f"Feature: {feature}, Importance: {importance}")


feature_importance = rf_best_classifier.feature_importances_
sorted_idx = feature_importance.argsort()
features = ['rating', 'genre', 'score', 'votes', 'budget', 'runtime', 'CPI', 'year', 'month', 'day', 'dayofweek', 'star_studded', 'top_company', 'top_director', 'top_writer']
plt.figure(figsize=(10, 8))
plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')
plt.yticks(range(len(sorted_idx)), [features[i] for i in sorted_idx])

plt.xlabel('Feature Importance')
plt.title('Feature Importance in Random Forest Classifier Predicting Success Using Grid Search CV')
plt.show()





from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score


xgb_reg = XGBRegressor(n_estimators=100, random_state=42)
xgb_reg.fit(X_train_gross, y_train_gross)
xgb_pred = xgb_reg.predict(X_test_gross)

mae_xgb = mean_absolute_error(y_test_gross, xgb_pred)
mse_xgb = mean_squared_error(y_test_gross, xgb_pred)
rmse_xgb = mean_squared_error(y_test_gross, xgb_pred, squared=False)
r2_xgb = r2_score(y_test_gross, xgb_pred)

print(f"Mean Absolute Error:, {mae_xgb}")
print(f"Mean Squared Error:, {mse_xgb}")
print(f"Root Mean Squared Error:, {rmse_xgb}")
print(f"R-squared:, {r2_xgb}")


feature_importance = xgb_reg.feature_importances_
feature_importance_with_columns = list(zip(X.columns, xgb_reg.feature_importances_))

sorted_feature_importance = sorted(feature_importance_with_columns, key=lambda x: x[1], reverse=True)

for feature, importance in sorted_feature_importance:
    print(f"Feature: {feature}, Importance: {importance}")


feature_importance = xgb_reg.feature_importances_
sorted_idx = feature_importance.argsort()
features = ['rating', 'genre', 'score', 'votes', 'budget', 'runtime', 'CPI', 'year', 'month', 'day', 'dayofweek', 'star_studded', 'top_company', 'top_director', 'top_writer']
plt.figure(figsize=(10, 8))
plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')
plt.yticks(range(len(sorted_idx)), [features[i] for i in sorted_idx])

plt.xlabel('Feature Importance')
plt.title('Feature Importance in Gradient Boosting for Regressor Predicting Gross Log')
plt.show()





from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

xgb_cls = XGBClassifier(n_estimators=100, random_state=42)
xgb_cls.fit(X_train_success, y_train_success)
xgb_pred_class = xgb_cls.predict(X_test_success)

accuracy = accuracy_score(y_test_success, xgb_pred_class)
precision = precision_score(y_test_success, xgb_pred_class)
recall = recall_score(y_test_success, xgb_pred_class)
f1 = f1_score(y_test_success, xgb_pred_class)
conf_matrix = confusion_matrix(y_test_success, xgb_pred_class)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1 Score: {f1}")
print(f"Confusion Matrix:\n{conf_matrix}")


feature_importance = xgb_cls.feature_importances_
feature_importance_with_columns = list(zip(X.columns, xgb_cls.feature_importances_))

sorted_feature_importance = sorted(feature_importance_with_columns, key=lambda x: x[1], reverse=True)

for feature, importance in sorted_feature_importance:
    print(f"Feature: {feature}, Importance: {importance}")


feature_importance = xgb_cls.feature_importances_
sorted_idx = feature_importance.argsort()
features = ['rating', 'genre', 'score', 'votes', 'budget', 'runtime', 'CPI', 'year', 'month', 'day', 'dayofweek', 'star_studded', 'top_company', 'top_director', 'top_writer']
plt.figure(figsize=(10, 8))
plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')
plt.yticks(range(len(sorted_idx)), [features[i] for i in sorted_idx])

plt.xlabel('Feature Importance')
plt.title('Feature Importance in Gradient Boosting for Classifier Predicting Success')
plt.show()


# Importance values for each feature across different categories
votes = [0.5149157348655525, 0.5700307879381045, 0.2754866760084887, 0.33768728361309786, 0.3186371922492981, 0.19338540732860565, 0.3547568763955934]
budget = [0.23996041400683363, 0.2605729570318566, 0.11698957030326226, 0.1159601906438301, 0.1159601906438301, 0.10160606354475021, 0.13785617464629896]
score = [0.03938769786621848, 0.026414445439291313, 0.10244215228374642, 0.10405206496344299, 0.10405206496344299, 0.07615770399570465, 0.08882447721255791]
day = [0.030056812163374316, 0.01934230749328846, 0.07718101190014205, 0.061042470677589146, 0.06365488058361506, 0.09229359785986137, 0.0]
runtime = [0.029872492419931524, 0.01798541163464375, 0.08811955861165793, 0.07873920056017943, 0.07873920056017943, 0.058316051959991455, 0.0]
year = [0.028922446530147278, 0.02045232206001599, 0.07262319401475943, 0.06702216286450541, 0.06702216286450541, 0.04333887617892185, 0.0]
month = [0.022990192751109948, 0.013978144397990545, 0.061575059651300876, 0.04892347480597939, 0.04892347480597939, 0.056169286370277405, 0.0]
dayofweek = [0.02192495211056887, 0.019512101658096685, 0.015974232189264213, 0.015284921589842081, 0.015284921589842081, 0.0070314587943782715, 0.0]
CPI = [0.021810973701890366, 0.014232017008596139, 0.0735355966238887, 0.06502808517545698, 0.06502808517545698, 0.07056299597024918, 0.0]
genre = [0.02069049739528874, 0.013085208724765391, 0.05102434878833597, 0.046752856714887846, 0.046752856714887846, 0.07181774824857712, 0.0]
rating = [0.018379072595571627, 0.015226260588228256, 0.032735622019295314, 0.030824282141255526, 0.030824282141255526, 0.09250235557556152, 0.0]
star_studded = [0.0033648335398659456, 0.0016030805091884332, 0.014947493682913022, 0.013195264871152947, 0.013195264871152947, 0.04713525250554085, 0.0]
top_company = [0.007723880053646835, 0.007564955515934008, 0.017365483922945263, 0.015487741378780364, 0.015487741378780364, 0.01924954331619059, 0.0]
top_director = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
top_writer = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

# Summing the importance values for each feature
total_votes = sum(votes)
total_budget = sum(budget)
total_score = sum(score)
total_day = sum(day)
total_runtime = sum(runtime)
total_year = sum(year)
total_month = sum(month)
total_dayofweek = sum(dayofweek)
total_CPI = sum(CPI)
total_genre = sum(genre)
total_rating = sum(rating)
total_star_studded = sum(star_studded)
total_top_company = sum(top_company)
total_top_director = sum(top_director)
total_top_writer = sum(top_writer)

# Displaying the summed importance values for each feature
print(f"votes = {total_votes}")
print(f"budget = {total_budget}")
print(f"score = {total_score}")
print(f"day = {total_day}")
print(f"runtime = {total_runtime}")
print(f"year = {total_year}")
print(f"month = {total_month}")
print(f"dayofweek = {total_dayofweek}")
print(f"CPI = {total_CPI}")
print(f"genre = {total_genre}")
print(f"rating = {total_rating}")
print(f"star_studded = {total_star_studded}")
print(f"top_company = {total_top_company}")
print(f"top_director = {total_top_director}")
print(f"top_writer = {total_top_writer}")






from sklearn.model_selection import train_test_split
X = merged_df[['rating', 'genre', 'score', 'votes', 'budget', 'runtime', 'CPI', 'year', 'month', 'day', 'dayofweek', 'star_studded', 'top_company', 'top_director', 'top_writer']]
y = merged_df['success']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


X


from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

clf = DecisionTreeClassifier()
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of CART model: {accuracy:.2f}")

# Calculate other evaluation metrics
conf_matrix = confusion_matrix(y_test, y_pred)
print(f"Confusion Matrix:\n{conf_matrix}")

# Classification Report
class_report = classification_report(y_test, y_pred)
print(f"Classification Report:\n{class_report}")


feature_importance = clf.feature_importances_
feature_importance_with_columns = list(zip(X.columns, clf.feature_importances_))

sorted_feature_importance = sorted(feature_importance_with_columns, key=lambda x: x[1], reverse=True)

for feature, importance in sorted_feature_importance:
    print(f"Feature: {feature}, Importance: {importance}")


feature_importance = clf.feature_importances_
sorted_idx = feature_importance.argsort()
features = ['rating', 'genre', 'score', 'votes', 'budget', 'runtime', 'CPI', 'year', 'month', 'day', 'dayofweek', 'star_studded', 'top_company', 'top_director', 'top_writer']
plt.figure(figsize=(10, 8))
plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')
plt.yticks(range(len(sorted_idx)), [features[i] for i in sorted_idx])

plt.xlabel('Feature Importance')
plt.title('Feature Importance in CART Predicting Success')
plt.show()





median_budget_by_genre = merged_df_copy.groupby('genre')['budget'].median()

genre_median_budget_dict = median_budget_by_genre.to_dict()

def impute_budget(row):
    if pd.isnull(row['budget']):
        return genre_median_budget_dict.get(row['genre'])
    else:
        return row['budget']

merged_df_copy['budget'] = merged_df_copy.apply(impute_budget, axis=1)
merged_df_copy


#cleaning
merged_df_copy['released'] = merged_df_copy['released'].astype(str).apply(lambda x: parser.parse(x, fuzzy=True))
merged_df_copy['released'] = pd.to_datetime(merged_df_copy['released'])

merged_df_copy['month'] = merged_df_copy['released'].dt.month
merged_df_copy['dayofweek'] = merged_df_copy['released'].apply(get_day_of_week)


merged_df_copy['rating'] = label_encoder.fit_transform(merged_df_copy['rating'])
merged_df_copy['genre'] = label_encoder.fit_transform(merged_df_copy['genre'])
merged_df_copy['company'] = label_encoder.fit_transform(merged_df_copy['company'])
merged_df_copy['dayofweek'] = label_encoder.fit_transform(merged_df_copy['dayofweek'])

print(merged_df_copy.isna().sum())
merged_df_copy.dropna(inplace=True)




from sklearn.model_selection import train_test_split
X = merged_df_copy[['rating', 'genre', 'score', 'votes', 'budget', 'company', 'runtime','CPI', 'month', 'dayofweek']]
y = merged_df_copy['success']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


clf2 = DecisionTreeClassifier()
clf2.fit(X_train, y_train)
y_pred = clf2.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of CART model: {accuracy:.2f}")

# Calculate other evaluation metrics
conf_matrix = confusion_matrix(y_test, y_pred)
print(f"Confusion Matrix:\n{conf_matrix}")

# Classification Report
class_report = classification_report(y_test, y_pred)
print(f"Classification Report:\n{class_report}")






rnn_df=merged_df.copy()


rnn_df



X = rnn_df.drop(columns = ['name','gross','country','gross_log','released'])

y =  rnn_df['gross_log']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train



scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
X_train_scaled = np.reshape(X_train_scaled, (X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))
X_test_scaled = np.reshape(X_test_scaled, (X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))
from keras.callbacks import EarlyStopping



model = Sequential()
model.add(LSTM(480, return_sequences=True, input_shape=(1, X_train_scaled.shape[2])))
model.add(Dropout(0.45))
model.add(LSTM(480, return_sequences=False))
model.add(Dropout(0.3))
model.add(Dense(1))  
early_stopping = EarlyStopping(monitor='val_loss', patience=14)

model.compile(optimizer='adam', loss='mean_squared_error')

model.fit(X_train_scaled, y_train, epochs=500, batch_size=128,callbacks=early_stopping ,validation_data=(X_test_scaled, y_test),verbose=1)

test_loss = model.evaluate(X_test_scaled, y_test)
predictions = model.predict(X_test_scaled)
mae_rnn = mean_squared_error(y_test, predictions)
mse_rnn = mean_squared_error(y_test, predictions)
rmse_rnn = mean_squared_error(y_test, predictions, squared=False)
r2_rnn = r2_score(y_test, predictions)

print(f"Mean Absolute Error:, {mae_rnn }")
print(f"Mean Squared Error:, {mse_rnn}")
print(f"Root Mean Squared Error:, {rmse_rnn}")
print(f"R-squared:, {r2_rnn}")









